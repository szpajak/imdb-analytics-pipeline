{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bfae45",
   "metadata": {},
   "source": [
    "# SBERT-based multilabel genre training pipeline\n",
    "\n",
    "This notebook loads cached datasets, builds SBERT embeddings once, and trains several multilabel classifiers (LightGBM One-vs-Rest, dense neural network, classifier chains) with per-class threshold tuning and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaaabd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8000, Test samples: 2000, Labels: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(DATA_DIR / 'movies_df.pkl', 'rb') as f:\n",
    "    movies_df = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR / 'train_data.pkl', 'rb') as f:\n",
    "    X_train, y_train, ids_train = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR / 'test_data.pkl', 'rb') as f:\n",
    "    X_test, y_test, ids_test = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR / 'mlb.pkl', 'rb') as f:\n",
    "    mlb = pickle.load(f)\n",
    "\n",
    "num_labels = y_train.shape[1]\n",
    "print(f'Train samples: {len(X_train)}, Test samples: {len(X_test)}, Labels: {num_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "X_train_emb_path = DATA_DIR / 'X_train_sbert.npy'\n",
    "X_test_emb_path = DATA_DIR / 'X_test_sbert.npy'\n",
    "\n",
    "def encode_texts(texts, model, batch_size=256):\n",
    "    encoded = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = list(texts[start:start + batch_size])\n",
    "        emb = model.encode(batch, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "        encoded.append(emb)\n",
    "    return np.vstack(encoded)\n",
    "\n",
    "if X_train_emb_path.exists() and X_test_emb_path.exists():\n",
    "    X_train_emb = np.load(X_train_emb_path)\n",
    "    X_test_emb = np.load(X_test_emb_path)\n",
    "    print('Loaded cached SBERT embeddings.')\n",
    "else:\n",
    "    sbert = SentenceTransformer(sbert_model_name)\n",
    "    X_train_emb = encode_texts(X_train, sbert)\n",
    "    X_test_emb = encode_texts(X_test, sbert)\n",
    "    np.save(X_train_emb_path, X_train_emb)\n",
    "    np.save(X_test_emb_path, X_test_emb)\n",
    "    print('Computed and cached SBERT embeddings.')\n",
    "\n",
    "print('Embedding shapes:', X_train_emb.shape, X_test_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be86e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: (6412, 384), Val split: (1588, 384)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    HAS_MSKF = True\n",
    "except ImportError:\n",
    "    HAS_MSKF = False\n",
    "\n",
    "indices = np.arange(X_train_emb.shape[0])\n",
    "if HAS_MSKF:\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_idx, val_idx = next(mskf.split(indices, y_train))\n",
    "else:\n",
    "    _, val_idx = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "    train_idx = np.setdiff1d(indices, val_idx)\n",
    "\n",
    "X_tr, X_val = X_train_emb[train_idx], X_train_emb[val_idx]\n",
    "y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "print(f'Train split: {X_tr.shape}, Val split: {X_val.shape}')\n",
    "\n",
    "def tune_thresholds(probs, y_true, grid=None):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.1, 0.9, 17)\n",
    "    thresholds = np.full(probs.shape[1], 0.5, dtype=np.float32)\n",
    "    for j in range(probs.shape[1]):\n",
    "        best_t, best_f1 = 0.5, 0.0\n",
    "        for t in grid:\n",
    "            preds = (probs[:, j] >= t).astype(int)\n",
    "            score = f1_score(y_true[:, j], preds, zero_division=0)\n",
    "            if score > best_f1:\n",
    "                best_f1 = score\n",
    "                best_t = t\n",
    "        thresholds[j] = best_t\n",
    "    return thresholds\n",
    "\n",
    "def evaluate_and_log(model_name, y_true, probs, thresholds, report_path):\n",
    "    thresholds = np.asarray(thresholds)\n",
    "    preds = (probs >= thresholds).astype(int)\n",
    "    metrics = {\n",
    "        'micro_f1': f1_score(y_true, preds, average='micro', zero_division=0),\n",
    "        'macro_f1': f1_score(y_true, preds, average='macro', zero_division=0),\n",
    "        'micro_precision': precision_score(y_true, preds, average='micro', zero_division=0),\n",
    "        'micro_recall': recall_score(y_true, preds, average='micro', zero_division=0)\n",
    "    }\n",
    "    report = classification_report(y_true, preds, target_names=list(mlb.classes_), zero_division=0)\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f'{model_name} metrics\\n')\n",
    "        for k, v in metrics.items():\n",
    "            f.write(f'{k}: {v}\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(report)\n",
    "    return metrics, report\n",
    "\n",
    "def save_thresholds(path, thresholds):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump({'classes': list(mlb.classes_), 'thresholds': [float(t) for t in thresholds]}, f, indent=2)\n",
    "\n",
    "def save_predictions(prefix, probs, thresholds):\n",
    "    thresholds = np.asarray(thresholds)\n",
    "    probs_path = MODELS_DIR / f'{prefix}_test_probs.npy'\n",
    "    preds_path = MODELS_DIR / f'{prefix}_test_preds.npy'\n",
    "    preds = (probs >= thresholds).astype(int)\n",
    "    np.save(probs_path, probs)\n",
    "    np.save(preds_path, preds)\n",
    "    return preds\n",
    "\n",
    "results_summary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf391a66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m lgbm_estimator = LGBMClassifier(\n\u001b[32m      6\u001b[39m     n_estimators=\u001b[32m800\u001b[39m,\n\u001b[32m      7\u001b[39m     learning_rate=\u001b[32m0.05\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m lgbm_ovr = OneVsRestClassifier(lgbm_estimator, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m lgbm_ovr.fit(\u001b[43mX_tr\u001b[49m, y_tr)\n\u001b[32m     19\u001b[39m val_probs_lgbm = lgbm_ovr.predict_proba(X_val)\n\u001b[32m     20\u001b[39m lgbm_thresholds = tune_thresholds(val_probs_lgbm, y_val)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from joblib import dump\n",
    "\n",
    "lgbm_estimator = LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm_ovr = OneVsRestClassifier(lgbm_estimator, n_jobs=-1)\n",
    "lgbm_ovr.fit(X_tr, y_tr)\n",
    "\n",
    "val_probs_lgbm = lgbm_ovr.predict_proba(X_val)\n",
    "lgbm_thresholds = tune_thresholds(val_probs_lgbm, y_val)\n",
    "\n",
    "test_probs_lgbm = lgbm_ovr.predict_proba(X_test_emb)\n",
    "lgbm_metrics, lgbm_report = evaluate_and_log(\n",
    "    'sbert_lgbm_ovr',\n",
    "    y_test,\n",
    "    test_probs_lgbm,\n",
    "    lgbm_thresholds,\n",
    "    MODELS_DIR / 'metrics_sbert_lgbm.txt'\n",
    ")\n",
    "\n",
    "dump(lgbm_ovr, MODELS_DIR / 'sbert_lgbm_ovr.pkl')\n",
    "save_thresholds(MODELS_DIR / 'sbert_lgbm_thresholds.json', lgbm_thresholds)\n",
    "save_predictions('sbert_lgbm_ovr', test_probs_lgbm, lgbm_thresholds)\n",
    "results_summary['sbert_lgbm_ovr'] = lgbm_metrics\n",
    "lgbm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594445f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_mlp(input_dim, output_dim):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "mlp_model = build_mlp(X_tr.shape[1], num_labels)\n",
    "mlp_model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0),\n",
    "    metrics=[tf.keras.metrics.AUC(name='auc', multi_label=True)]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = mlp_model.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "val_probs_mlp = mlp_model.predict(X_val, batch_size=256)\n",
    "mlp_thresholds = tune_thresholds(val_probs_mlp, y_val)\n",
    "\n",
    "test_probs_mlp = mlp_model.predict(X_test_emb, batch_size=256)\n",
    "mlp_metrics, mlp_report = evaluate_and_log(\n",
    "    'sbert_mlp_dense',\n",
    "    y_test,\n",
    "    test_probs_mlp,\n",
    "    mlp_thresholds,\n",
    "    MODELS_DIR / 'metrics_sbert_mlp.txt'\n",
    ")\n",
    "\n",
    "mlp_model.save(MODELS_DIR / 'sbert_mlp_dense.keras')\n",
    "save_thresholds(MODELS_DIR / 'sbert_mlp_thresholds.json', mlp_thresholds)\n",
    "save_predictions('sbert_mlp_dense', test_probs_mlp, mlp_thresholds)\n",
    "results_summary['sbert_mlp_dense'] = mlp_metrics\n",
    "mlp_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "chain_seeds = [42, 52, 62]\n",
    "chains = []\n",
    "for seed in chain_seeds:\n",
    "    base = LogisticRegression(\n",
    "        max_iter=500,\n",
    "        C=2.0,\n",
    "        solver='saga',\n",
    "        penalty='l2',\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced',\n",
    "        random_state=seed\n",
    "    )\n",
    "    chain = ClassifierChain(base_estimator=base, order='random', random_state=seed)\n",
    "    chain.fit(X_tr, y_tr)\n",
    "    chains.append(chain)\n",
    "\n",
    "def chain_predict_proba(chain_list, X):\n",
    "    probs = [chain.predict_proba(X) for chain in chain_list]\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "val_probs_chain = chain_predict_proba(chains, X_val)\n",
    "chain_thresholds = tune_thresholds(val_probs_chain, y_val)\n",
    "\n",
    "test_probs_chain = chain_predict_proba(chains, X_test_emb)\n",
    "chain_metrics, chain_report = evaluate_and_log(\n",
    "    'sbert_classifier_chains',\n",
    "    y_test,\n",
    "    test_probs_chain,\n",
    "    chain_thresholds,\n",
    "    MODELS_DIR / 'metrics_sbert_classifier_chains.txt'\n",
    ")\n",
    "\n",
    "dump(chains, MODELS_DIR / 'sbert_classifier_chains.pkl')\n",
    "save_thresholds(MODELS_DIR / 'sbert_classifier_chains_thresholds.json', chain_thresholds)\n",
    "save_predictions('sbert_classifier_chains', test_probs_chain, chain_thresholds)\n",
    "results_summary['sbert_classifier_chains'] = chain_metrics\n",
    "chain_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714044b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(results_summary).T\n",
    "summary_path = MODELS_DIR / 'metrics_sbert_models_summary.csv'\n",
    "summary_df.to_csv(summary_path)\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
